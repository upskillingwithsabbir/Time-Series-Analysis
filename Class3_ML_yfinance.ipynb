{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fc320e",
   "metadata": {},
   "source": [
    "# Class 3: Time Series Analysis with ML Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa47300",
   "metadata": {},
   "source": [
    "This notebook explores Machine Learning approaches for time series forecasting, focusing on Facebook Prophet and XGBoost. We will also discuss appropriate train-test split strategies for time series data and compare the results with traditional statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4806f3b",
   "metadata": {},
   "source": [
    "## 0. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "from prophet import Prophet\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "plt.rcParams['axes.grid'] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b4077",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Train-Test Split for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b816ef",
   "metadata": {},
   "source": [
    "Train-Test Split concept not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download 10 years of Apple stock data\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2025-01-01'\n",
    "ticker = 'AAPL'\n",
    "\n",
    "# Fetch data using yfinance\n",
    "df = yf.download(ticker, start=start_date, end=end_date)\n",
    "print(f\"Downloaded {ticker} stock data from {start_date} to {end_date}\")\n",
    "print(f\"Shape of data: {df.shape}\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst 5 rows of the data:\")\n",
    "display(df.head())\n",
    "\n",
    "# We'll focus on the 'Adj Close' price for our analysis\n",
    "ts = df['Adj Close']\n",
    "print(f\"\\nFocusing on Adjusted Close price for {ticker}\")\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "train_size = int(len(ts) * 0.8)\n",
    "train_ts = ts[:train_size]\n",
    "test_ts = ts[train_size:]\n",
    "print(f\"\\nSplit data into training ({len(train_ts)} samples) and testing ({len(test_ts)} samples) sets\")\n",
    "\n",
    "# Plot the training and testing data\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(train_ts, label='Training Data')\n",
    "plt.plot(test_ts, label='Testing Data')\n",
    "plt.title(f'{ticker} Stock Price - Train/Test Split')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- 1. Loading Data ---\")\n",
    "print(f\"Train set size: {len(train_ts)}\")\n",
    "print(f\"Test set size: {len(test_ts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883165d",
   "metadata": {},
   "source": [
    "## 2. Facebook Prophet for Time Series Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe38dd",
   "metadata": {},
   "source": [
    "Prophet/ML Approach concept not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f305775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- 2. Fitting Prophet Model ---\")\n",
    "\n",
    "# Prepare data for Prophet (requires 'ds' for dates and 'y' for values)\n",
    "prophet_train = pd.DataFrame({\n",
    "    'ds': train_ts.index,\n",
    "    'y': train_ts.values\n",
    "})\n",
    "\n",
    "# Create and fit Prophet model\n",
    "try:\n",
    "    model = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.05  # Flexibility of the trend\n",
    "    )\n",
    "    model.fit(prophet_train)\n",
    "    \n",
    "    # Create future dataframe for prediction (using the test period)\n",
    "    future_dates = model.make_future_dataframe(\n",
    "        periods=len(test_ts),\n",
    "        freq='B'  # Business days frequency\n",
    "    )\n",
    "    \n",
    "    # Ensure future_dates aligns with test_ts index\n",
    "    future_dates = future_dates[future_dates['ds'] <= test_ts.index[-1]]\n",
    "    \n",
    "    # Make predictions\n",
    "    forecast = model.predict(future_dates)\n",
    "    \n",
    "    # Extract predictions for the test period\n",
    "    prophet_forecast = forecast[forecast['ds'] >= test_ts.index[0]]['yhat'].values\n",
    "    \n",
    "    # Ensure forecast length matches test set\n",
    "    if len(prophet_forecast) != len(test_ts):\n",
    "        # Reindex to match test dates exactly\n",
    "        forecast_df = pd.DataFrame({'ds': forecast['ds'], 'yhat': forecast['yhat']})\n",
    "        forecast_df = forecast_df.set_index('ds')\n",
    "        prophet_forecast = forecast_df.reindex(test_ts.index).values.flatten()\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    prophet_rmse = math.sqrt(mean_squared_error(test_ts, prophet_forecast))\n",
    "    prophet_mae = mean_absolute_error(test_ts, prophet_forecast)\n",
    "    print(f\"Prophet RMSE: {prophet_rmse:.4f}\")\n",
    "    print(f\"Prophet MAE: {prophet_mae:.4f}\")\n",
    "    \n",
    "    # Plot the forecast\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(train_ts, label='Training Data')\n",
    "    plt.plot(test_ts, label='Actual Test Data')\n",
    "    plt.plot(test_ts.index, prophet_forecast, label='Prophet Forecast', color='red')\n",
    "    plt.title('Facebook Prophet Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plot_15_prophet_forecast.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Prophet components\n",
    "    fig = model.plot_components(forecast)\n",
    "    plt.savefig('plot_16_prophet_components.png')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error fitting Prophet: {e}\")\n",
    "    prophet_rmse = float('nan')\n",
    "    prophet_mae = float('nan')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db631698",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "\n",
    "Facebook Prophet is designed for forecasting time series data with strong seasonal patterns and multiple seasonality levels:\n",
    "\n",
    "Key Features:\n",
    "1. Automatic decomposition into trend, seasonality, and holidays\n",
    "2. Handles missing data and outliers robustly\n",
    "3. Can incorporate holiday effects and special events\n",
    "4. Provides uncertainty intervals for forecasts\n",
    "5. Designed to be easy to use with minimal parameter tuning\n",
    "\n",
    "The model components plot shows:\n",
    "- Trend: The overall direction of the time series\n",
    "- Yearly Seasonality: Patterns that repeat annually\n",
    "- Weekly Seasonality: Patterns that repeat weekly\n",
    "- Daily Seasonality (if enabled): Patterns that repeat daily\n",
    "\n",
    "Prophet works well for:\n",
    "- Business forecasting tasks with strong seasonality\n",
    "- Time series with multiple seasonal patterns\n",
    "- Data with missing values or outliers\n",
    "- Forecasts that need to incorporate known future events\n",
    "\n",
    "Note: For financial time series like stock prices, Prophet may not always outperform other models since stock prices often don't follow regular seasonal patterns and are influenced by many external factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337920b",
   "metadata": {},
   "source": [
    "## 3. XGBoost for Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc50c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- 3. Fitting XGBoost Model ---\")\n",
    "\n",
    "# Feature engineering for XGBoost\n",
    "def create_features(df):\n",
    "    # Create time series features based on time series index\n",
    "    df = df.copy()\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['month'] = df.index.month\n",
    "    df['year'] = df.index.year\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    df['dayofmonth'] = df.index.day\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    return df\n",
    "\n",
    "# Create a dataframe with the time series\n",
    "df_full = pd.DataFrame({'y': ts})\n",
    "\n",
    "# Add time-based features\n",
    "df_full = create_features(df_full)\n",
    "\n",
    "# Add lag features (must be done before train-test split to avoid lookahead bias)\n",
    "for lag in [1, 5, 10, 21]:  # 1 day, 1 week, 2 weeks, 1 month\n",
    "    df_full[f'lag_{lag}'] = df_full['y'].shift(lag)\n",
    "\n",
    "# Add rolling mean features\n",
    "for window in [5, 21]:  # 1 week, 1 month\n",
    "    df_full[f'rolling_mean_{window}'] = df_full['y'].rolling(window=window).mean()\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df = df_full[:train_size].copy()\n",
    "test_df = df_full[train_size:].copy()\n",
    "\n",
    "# Drop NaN values (created by lag and rolling features)\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "# Define features and target\n",
    "feature_columns = [col for col in train_df.columns if col != 'y']\n",
    "X_train = train_df[feature_columns]\n",
    "y_train = train_df['y']\n",
    "X_test = test_df[feature_columns]\n",
    "y_test = test_df['y']\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Use early stopping to prevent overfitting\n",
    "eval_set = [(X_train, y_train)]\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=eval_set,\n",
    "    eval_metric='rmse',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "xgb_forecast = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate error metrics\n",
    "xgb_rmse = math.sqrt(mean_squared_error(y_test, xgb_forecast))\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_forecast)\n",
    "print(f\"XGBoost RMSE: {xgb_rmse:.4f}\")\n",
    "print(f\"XGBoost MAE: {xgb_mae:.4f}\")\n",
    "\n",
    "# Plot the forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(train_ts, label='Training Data')\n",
    "plt.plot(test_ts, label='Actual Test Data')\n",
    "plt.plot(test_ts.index, xgb_forecast, label='XGBoost Forecast', color='green')\n",
    "plt.title('XGBoost Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot_17_xgboost_forecast.png')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "xgb.plot_importance(xgb_model, max_num_features=10)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4902e3",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a powerful machine learning algorithm that can be adapted for time series forecasting:\n",
    "\n",
    "Key Features:\n",
    "1. Gradient boosting framework that builds an ensemble of decision trees\n",
    "2. Can capture complex non-linear relationships\n",
    "3. Handles a mix of numerical and categorical features\n",
    "4. Provides feature importance rankings\n",
    "5. Regularization to prevent overfitting\n",
    "\n",
    "For time series forecasting with XGBoost:\n",
    "- Feature engineering is crucial (time-based features, lag features, rolling statistics)\n",
    "- The model treats forecasting as a supervised learning problem\n",
    "- Early stopping helps prevent overfitting\n",
    "- Feature importance helps identify which factors most influence the predictions\n",
    "\n",
    "XGBoost works well for:\n",
    "- Complex time series with non-linear relationships\n",
    "- Forecasting problems where external variables are important\n",
    "- Cases where interpretability of feature importance is valuable\n",
    "- Situations where traditional time series models underperform\n",
    "\n",
    "The feature importance plot shows which features contribute most to the predictions, which can provide valuable insights about the time series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163f2cf",
   "metadata": {},
   "source": [
    "## 4. Comparing Statistical and ML Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870081ab",
   "metadata": {},
   "source": [
    "Comparison concept not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- 4. ML Model Performance Comparison (Test Set) ---\")\n",
    "print(f\"Prophet RMSE: {prophet_rmse:.4f}, MAE: {prophet_mae:.4f}\")\n",
    "print(f\"XGBoost RMSE: {xgb_rmse:.4f}, MAE: {xgb_mae:.4f}\")\n",
    "print(\"Compare these metrics with those from Class 2 (Statistical Models).\")\n",
    "\n",
    "# Note: In a real-world scenario, you would load the results from Class 2 models\n",
    "# and create a comprehensive comparison here. For this notebook, we'll just\n",
    "# compare the ML models we've implemented.\n",
    "\n",
    "# Determine the best ML model\n",
    "if not np.isnan(prophet_rmse) and not np.isnan(xgb_rmse):\n",
    "    best_model = \"Prophet\" if prophet_rmse < xgb_rmse else \"XGBoost\"\n",
    "    print(f\"\\nBest performing ML model based on RMSE: {best_model}\")\n",
    "elif not np.isnan(xgb_rmse):\n",
    "    print(\"\\nXGBoost is the only successfully trained ML model\")\n",
    "elif not np.isnan(prophet_rmse):\n",
    "    print(\"\\nProphet is the only successfully trained ML model\")\n",
    "else:\n",
    "    print(\"\\nNo ML models were successfully trained and evaluated\")\n",
    "\n",
    "print(\"\\nClass 3 Demonstrations Complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e06f1d",
   "metadata": {},
   "source": [
    "**Comprehensive Comparison:**\n",
    "\n",
    "\n",
    "Comparing Statistical and Machine Learning approaches for time series forecasting reveals important insights:\n",
    "\n",
    "### Statistical Models (ARIMA/SARIMAX):\n",
    "- **Strengths:**\n",
    "  - Strong theoretical foundation based on time series properties\n",
    "  - Interpretable parameters with clear statistical meaning\n",
    "  - Effective for data with clear autocorrelation patterns\n",
    "  - Relatively simple to implement for basic cases\n",
    "  - Provide confidence intervals with statistical guarantees\n",
    "\n",
    "- **Limitations:**\n",
    "  - Rely on stationarity assumptions\n",
    "  - May struggle with complex non-linear patterns\n",
    "  - Limited ability to incorporate many external variables\n",
    "  - Manual order selection can be time-consuming\n",
    "  - May not handle multiple seasonality well\n",
    "\n",
    "### Machine Learning Models (Prophet/XGBoost):\n",
    "- **Strengths:**\n",
    "  - Can capture complex non-linear relationships\n",
    "  - Flexible incorporation of many features and external variables\n",
    "  - Prophet handles multiple seasonality patterns automatically\n",
    "  - XGBoost provides feature importance for interpretability\n",
    "  - Often require less manual parameter tuning\n",
    "\n",
    "- **Limitations:**\n",
    "  - May require more data for effective training\n",
    "  - Risk of overfitting with too many features\n",
    "  - Less theoretical foundation in time series properties\n",
    "  - May be computationally more intensive\n",
    "  - Feature engineering is crucial for good performance\n",
    "\n",
    "### When to Use Each Approach:\n",
    "- **Statistical Models:** When you have clear autocorrelation patterns, need interpretability, have limited data, or when the time series follows traditional patterns.\n",
    "- **Prophet:** When dealing with time series with multiple seasonality patterns, missing data, or when you need an easy-to-use forecasting tool.\n",
    "- **XGBoost:** When you have many potential predictive features, complex non-linear relationships, or when traditional models underperform.\n",
    "\n",
    "### Hybrid Approaches:\n",
    "- Combining statistical and ML models can leverage the strengths of both\n",
    "- Use statistical models for baseline forecasts and ML for residual modeling\n",
    "- Ensemble methods can combine predictions from multiple model types\n",
    "- Feature engineering informed by statistical analysis can improve ML models\n",
    "\n",
    "The best approach often depends on the specific characteristics of your time series data, the forecasting horizon, and your specific requirements for interpretability versus accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a082d",
   "metadata": {},
   "source": [
    "## Final Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8cb4a4",
   "metadata": {},
   "source": [
    "\n",
    "Based on our exploration of both statistical and machine learning approaches for time series forecasting, here are key recommendations:\n",
    "\n",
    "1. **Start Simple, Then Increase Complexity:**\n",
    "   - Begin with simple models like ARIMA as a baseline\n",
    "   - Add complexity only if simpler models don't perform adequately\n",
    "   - Compare performance metrics across model types\n",
    "\n",
    "2. **Proper Evaluation:**\n",
    "   - Always use a proper time series train-test split (no future leakage)\n",
    "   - Consider multiple error metrics (RMSE, MAE)\n",
    "   - Evaluate models across different forecast horizons\n",
    "   - Test model performance during different market conditions\n",
    "\n",
    "3. **Feature Engineering:**\n",
    "   - For ML models, feature engineering is crucial\n",
    "   - Include calendar features (day of week, month, holidays)\n",
    "   - Create lag features at appropriate intervals\n",
    "   - Add rolling statistics (means, standard deviations)\n",
    "   - Consider domain-specific external variables\n",
    "\n",
    "4. **Model Selection Considerations:**\n",
    "   - For data with clear seasonal patterns: SARIMAX or Prophet\n",
    "   - For data with many potential predictive features: XGBoost\n",
    "   - For volatility forecasting: GARCH models\n",
    "   - For interpretability needs: Statistical models or simpler ML models\n",
    "\n",
    "5. **Practical Implementation:**\n",
    "   - Regularly retrain models as new data becomes available\n",
    "   - Implement monitoring to detect when model performance degrades\n",
    "   - Consider ensemble approaches combining multiple models\n",
    "   - Balance complexity with maintainability for production systems\n",
    "\n",
    "6. **For Stock Price Forecasting Specifically:**\n",
    "   - Recognize the inherent unpredictability of financial markets\n",
    "   - Focus on probabilistic forecasts rather than point estimates\n",
    "   - Consider incorporating sentiment data and market indicators\n",
    "   - Combine price forecasts with volatility forecasts for risk assessment\n",
    "   - Remember that even the best models have limitations in financial forecasting\n",
    "\n",
    "The most effective approach often combines elements from both statistical and machine learning methods, leveraging the strengths of each while mitigating their weaknesses.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
